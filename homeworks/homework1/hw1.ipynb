{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rUp2xWkdGFu"
   },
   "source": [
    "# Autoregressive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zk6rWePvdGFv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import io\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-wuVhhNdGFz"
   },
   "source": [
    "Use the following functions to train your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U3MMoLe6dGFz"
   },
   "outputs": [],
   "source": [
    "def get_cross_entropy_loss(model, x):\n",
    "    # your code\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, use_cuda):\n",
    "    model.train()\n",
    "  \n",
    "    train_losses = []\n",
    "    for x in train_loader:\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        loss = get_cross_entropy_loss(model, x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    return train_losses\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader, use_cuda):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x in data_loader:\n",
    "            if use_cuda:\n",
    "                x = x.cuda()\n",
    "            loss = get_cross_entropy_loss(model, x)\n",
    "            total_loss += loss * x.shape[0]\n",
    "        avg_loss = total_loss / len(data_loader.dataset)\n",
    "    return avg_loss.item()\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs, lr, use_cuda=False):\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = [eval_model(model, test_loader, use_cuda)]\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses.extend(train_epoch(model, train_loader, optimizer, use_cuda))\n",
    "        test_loss = eval_model(model, test_loader, use_cuda)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ak7kqNGfdGF2"
   },
   "source": [
    "## Task 1: 1D histogram\n",
    "\n",
    "In this task you should train histogram model on discrete 1D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HafgFHSodGF3"
   },
   "outputs": [],
   "source": [
    "def generate_1d_data(count, bins):\n",
    "    np.random.seed(42)\n",
    "    a = 0.2 + 0.05 * np.random.randn(count)\n",
    "    b = 0.6 + 0.15 * np.random.randn(count)\n",
    "    mask = np.random.rand(count) < 0.5\n",
    "    samples = (a * mask + b * (1 - mask)) * (bins - 1)\n",
    "    data = np.clip(samples.astype('int'), 0.0, (bins - 1))\n",
    "    split = int(0.8 * count)\n",
    "    train_data, test_data = data[:split], data[split:]\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def plot_1d_data(train_data, test_data):\n",
    "    bins = int(max(test_data.max(), train_data.max()) - min(test_data.min(), train_data.min())) + 1\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax1.set_title('train')\n",
    "    ax1.hist(train_data, bins=bins, density=True)\n",
    "    ax1.set_xlabel('x')\n",
    "    ax2.set_title('test')\n",
    "    ax2.hist(test_data, bins=np.arange(bins), density=True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_training_curves(train_losses, test_losses):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    n_epochs = len(test_losses) - 1\n",
    "    x_train = np.linspace(0, n_epochs, len(train_losses))\n",
    "    x_test = np.arange(n_epochs + 1)\n",
    "\n",
    "    plt.plot(x_train, train_losses, label='train loss')\n",
    "    plt.plot(x_test, test_losses, label='test loss')\n",
    "    plt.legend()\n",
    "    plt.title('training curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('NLL')\n",
    "    \n",
    "    \n",
    "def plot_1d_distribution(data, distribution):\n",
    "    size = len(distribution)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(data, bins=np.arange(size) - 0.5, label='train data', density=True)\n",
    "\n",
    "    x = np.linspace(-0.5, size - 0.5, 1000)\n",
    "    y = distribution.repeat(1000 // size)\n",
    "    plt.plot(x, y, label='learned distribution')\n",
    "\n",
    "    plt.title('learned distribution')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "Ri9wZ3ZAdGF5",
    "outputId": "148e500c-4bbe-4a83-be63-09dceb4db197"
   },
   "outputs": [],
   "source": [
    "BINS = 100\n",
    "COUNT = 5000\n",
    "\n",
    "train_data, test_data = generate_1d_data(COUNT, BINS)\n",
    "plot_1d_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFSdE1E8dGF9"
   },
   "source": [
    "Let $\\theta = (\\theta_0, \\dots, \\theta_{d}) \\in \\mathbb{R}^{d}$ are model parameters. Your model is a softmax function: $p(k| \\theta) = \\frac{e^{\\theta_k}}{\\sum_{l=1}^d e^{\\theta_{l}}}$.\n",
    "\n",
    "The goal is to train $p(k| \\theta)$ using maximum likelihood via stochastic gradient descent on the training set, using $\\theta$ initialized to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJwGQ_SsdGF-"
   },
   "outputs": [],
   "source": [
    "class SoftmaxModel(nn.Module):\n",
    "    def __init__(self, bins):\n",
    "        super().__init__()\n",
    "        self.bins = bins\n",
    "        self.logits = nn.Parameter(torch.zeros(bins))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # your code\n",
    "        \n",
    "        \n",
    "def get_distribution(model):\n",
    "    # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 791
    },
    "colab_type": "code",
    "id": "ZdPLQARbdGGA",
    "outputId": "d2f18699-b030-4c7d-fd18-9b400aa27e35"
   },
   "outputs": [],
   "source": [
    "# you have to choose these parameters by yourself\n",
    "BATCH_SIZE = \n",
    "EPOCHS = \n",
    "LR = \n",
    "USE_CUDA = \n",
    "\n",
    "model = SoftmaxModel(BINS)\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "train_losses, test_losses = train_model(model, train_loader, test_loader, epochs=EPOCHS, lr=LR, use_cuda=USE_CUDA)\n",
    "distribution = get_distribution(model)\n",
    "\n",
    "plot_training_curves(train_losses, test_losses)\n",
    "plot_1d_distribution(train_data, distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4GLIbwqkdGGD"
   },
   "source": [
    "## Task 2: MADE\n",
    "\n",
    "Train MADE model on single image (https://arxiv.org/abs/1502.03509)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ngmqoNA2dGGD"
   },
   "outputs": [],
   "source": [
    "def generate_2d_data(count, bins):\n",
    "    im = Image.open('dgm.png').resize((bins, bins)).convert('L')\n",
    "    im = np.array(im).astype('float32')\n",
    "    dist = im / im.sum()\n",
    "\n",
    "    pairs = list(itertools.product(range(bins), range(bins)))\n",
    "    idxs = np.random.choice(len(pairs), size=count, replace=True, p=dist.reshape(-1))\n",
    "    samples = np.array([pairs[i] for i in idxs])\n",
    "\n",
    "    split = int(0.8 * len(samples))\n",
    "    return dist, samples[:split], samples[split:]\n",
    "\n",
    "\n",
    "def plot_2d_data(train_data, test_data):\n",
    "    bins = int(max(test_data.max(), train_data.max()) - min(test_data.min(), train_data.min())) + 1\n",
    "    train_dist, test_dist = np.zeros((bins, bins)), np.zeros((bins, bins))\n",
    "    for i in range(len(train_data)):\n",
    "        train_dist[train_data[i][0], train_data[i][1]] += 1\n",
    "    train_dist /= train_dist.sum()\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        test_dist[test_data[i][0], test_data[i][1]] += 1\n",
    "    test_dist /= test_dist.sum()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\n",
    "    ax1.set_title('Train Data')\n",
    "    ax1.imshow(train_dist, cmap='gray')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_xlabel('x1')\n",
    "    ax1.set_ylabel('x0')\n",
    "\n",
    "    ax2.set_title('Test Data')\n",
    "    ax2.imshow(test_dist, cmap='gray')\n",
    "    ax2.axis('off')\n",
    "    ax2.set_xlabel('x1')\n",
    "    ax2.set_ylabel('x0')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_2d_distribution(true_dist, learned_dist):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "    ax1.imshow(true_dist, cmap='gray')\n",
    "    ax1.set_title('True Distribution')\n",
    "    ax1.axis('off')\n",
    "    ax2.imshow(learned_dist, cmap='gray')\n",
    "    ax2.set_title('Learned Distribution')\n",
    "    ax2.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "eX5p_02jdGGG",
    "outputId": "76fbb984-7071-4944-80dc-a4dd6e8af425"
   },
   "outputs": [],
   "source": [
    "COUNT = 20000\n",
    "BINS = 60\n",
    "\n",
    "image, train_data, test_data = generate_2d_data(COUNT, BINS)\n",
    "plot_2d_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R--RMmRgdGGI"
   },
   "outputs": [],
   "source": [
    "def to_one_hot(labels, d):\n",
    "    one_hot = torch.FloatTensor(labels.shape[0], d).to(labels.device)\n",
    "    one_hot.zero_()\n",
    "    one_hot.scatter_(1, labels.unsqueeze(1), 1)\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "class MaskedLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.register_buffer('mask', torch.ones(out_features, in_features))\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.mask.data.copy_(torch.from_numpy(mask.astype(np.uint8).T))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.mask * self.weight, self.bias)\n",
    "\n",
    "    \n",
    "class MADE(nn.Module):\n",
    "    def __init__(self, nin, bins, hidden_sizes):\n",
    "        super().__init__()\n",
    "        self.nin = nin\n",
    "        self.nout = self.nin * bins\n",
    "        self.bins = bins\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.ordering = np.arange(self.nin)\n",
    "\n",
    "        # define a simple MLP neural net\n",
    "        self.net = # your code\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "        self.create_mask()  # builds the initial self.m connectivity\n",
    "\n",
    "        \n",
    "    def create_mask(self):\n",
    "        self.masks = []\n",
    "        # your code\n",
    "        self.masks = # your code\n",
    "\n",
    "        # set the masks in all MaskedLinear layers\n",
    "        layers = [l for l in self.net.modules() if isinstance(l, MaskedLinear)]\n",
    "        for l, m in zip(layers, masks):\n",
    "            l.set_mask(m)\n",
    "\n",
    "    def visualize_masks(self):\n",
    "        for m in self.masks:\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            plt.imshow(m, cmap='gray')\n",
    "            plt.show()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # your code\n",
    "\n",
    "\n",
    "def get_distribution(model):\n",
    "    # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_NWRhm_zdGGK",
    "outputId": "15baafb5-b110-4899-9824-64fb7903b2c7"
   },
   "outputs": [],
   "source": [
    "# you have to choose these parameters by yourself\n",
    "BATCH_SIZE = \n",
    "EPOCHS = \n",
    "LR = \n",
    "USE_CUDA = \n",
    "HIDDEN_SIZES = \n",
    "\n",
    "\n",
    "model = MADE(2, BINS, HIDDEN_SIZES)\n",
    "model.visualize_masks()\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "train_losses, test_losses = train_model(model, train_loader, test_loader, epochs=EPOCHS, lr=LR, use_cuda=USE_CUDA)\n",
    "\n",
    "distribution = get_distribution(model)\n",
    "plot_training_curves(train_losses, test_losses)\n",
    "plot_2d_distribution(image, distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l0p2m-MqcBXW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "hw1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
