{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VpbHkgMddS4"
   },
   "source": [
    "# Latent Variable Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00U99CI9X7GA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PksKzhF733Np"
   },
   "source": [
    "Below you could see the functions which visualizes 2d data and plots the obtained training curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqmHx9yW3SSH"
   },
   "outputs": [],
   "source": [
    "def visualize_2d_data(train_data, test_data, train_labels=None, test_labels=None):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax1.set_title('train')\n",
    "    ax1.scatter(train_data[:, 0], train_data[:, 1], s=1, c=train_labels)\n",
    "    ax2.set_title('test')\n",
    "    ax2.scatter(test_data[:, 0], test_data[:, 1], s=1, c=test_labels)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_2d_samples(data, title):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.scatter(data[:, 0], data[:, 1])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_training_curves(train_losses, test_losses):\n",
    "    n_train = len(train_losses[list(train_losses.keys())[0]])\n",
    "    n_test = len(test_losses[list(train_losses.keys())[0]])\n",
    "    x_train = np.linspace(0, n_test - 1, n_train)\n",
    "    x_test = np.arange(n_test)\n",
    "\n",
    "    plt.figure()\n",
    "    for key, value in train_losses.items():\n",
    "        plt.plot(x_train, value, label=key + '_train')\n",
    "\n",
    "    for key, value in test_losses.items():\n",
    "        plt.plot(x_test, value, label=key + '_test')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AieobeyB372L"
   },
   "source": [
    "Here are the functions that we will you for training out model. Please, explore these functions carefully. You do not have to change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krGVH3SN3v_g"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, epoch, loss_key='total'):\n",
    "    model.train()\n",
    "    stats = defaultdict(list)\n",
    "    for x in train_loader:\n",
    "        x = x.cuda()\n",
    "        losses = model.loss(x)\n",
    "        optimizer.zero_grad()\n",
    "        losses[loss_key].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for k, v in losses.items():\n",
    "            stats[k].append(v.item())\n",
    "    return stats\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    stats = defaultdict(float)\n",
    "    with torch.no_grad():\n",
    "        for x in data_loader:\n",
    "            x = x.cuda()\n",
    "            losses = model.loss(x)\n",
    "            for k, v in losses.items():\n",
    "                stats[k] += v.item() * x.shape[0]\n",
    "\n",
    "        for k in stats.keys():\n",
    "            stats[k] /= len(data_loader.dataset)\n",
    "    return stats\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs, lr, loss_key='total'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = defaultdict(list)\n",
    "    test_losses = defaultdict(list)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, epoch, loss_key)\n",
    "        test_loss = eval_model(model, test_loader)\n",
    "\n",
    "        for k in train_loss.keys():\n",
    "            train_losses[k].extend(train_loss[k])\n",
    "            test_losses[k].append(test_loss[k])\n",
    "    return dict(train_losses), dict(test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZjBDpnudlJL"
   },
   "source": [
    "## Task 1: VAE on 2d data\n",
    "\n",
    "In this task you will implement simple VAE model for 2d gaussian distribution.\n",
    "\n",
    "We will consider two cases: 2d univariate distribution and 2d multivariate distribution. The goal is to analyze the difference between these two cases and chosen VAE model. You do not have to change data generation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdTy9DLja_jV"
   },
   "outputs": [],
   "source": [
    "def generate_2d_data(count, mode='univariate'):\n",
    "    assert mode in ['univariate', 'multivariate']\n",
    "    np.random.seed(42)\n",
    "    mean = [[2.0, 3.0]]\n",
    "    sigma = [[3.0, 1.0]]\n",
    "    if mode == 'univariate':\n",
    "        rotate = [\n",
    "            [1.0, 0.0], \n",
    "            [0.0, 1.0]\n",
    "        ]\n",
    "    else:\n",
    "        rotate = [\n",
    "            [np.sqrt(2) / 2, np.sqrt(2) / 2], \n",
    "            [-np.sqrt(2) / 2, np.sqrt(2) / 2]\n",
    "        ]\n",
    "    data = mean + (np.random.randn(count, 2) * sigma).dot(rotate)\n",
    "    data = data.astype('float32')\n",
    "    split = int(0.8 * count)\n",
    "    train_data, test_data = data[:split], data[split:]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 655
    },
    "id": "JlZlxRm3a8Mi",
    "outputId": "079570fc-bac5-40a2-9579-06a0a0dd9468"
   },
   "outputs": [],
   "source": [
    "COUNT = 10000\n",
    "\n",
    "train_data, test_data = generate_2d_data(COUNT, mode='multivariate')\n",
    "visualize_2d_data(train_data, test_data)\n",
    "\n",
    "train_data, test_data = generate_2d_data(COUNT, mode='univariate')\n",
    "visualize_2d_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biYy9_rWd-DY"
   },
   "source": [
    "Now it is time to define our model. In this task you will create VAE model on 2d data. Model will be designed as:\n",
    "\n",
    "* the latent dimensionality is equal to 2 (the same as the data dimensionality).\n",
    "* prior distribution $p(\\mathbf{z}) = \\mathcal{N}(0, I)$.\n",
    "* approximate variational distribution (or encoder) $q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\mathbf{x}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x}))$. \n",
    "* posterior distribution (or decoder) $p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}))$.\n",
    "* Here the means and the covariance matrices are neural networks. Moreover, we will consider only diagonal covariance matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KN7NnFplkUSn"
   },
   "outputs": [],
   "source": [
    "class FullyConnectedMLP(nn.Module):\n",
    "    def __init__(self, input_shape, hiddens, output_shape):\n",
    "        assert isinstance(hiddens, list)\n",
    "        super().__init__()\n",
    "        self.input_shape = (input_shape,)\n",
    "        self.output_shape = (output_shape,)\n",
    "        self.hiddens = hiddens\n",
    "\n",
    "        # Stack some fully connected layers wirh relu activation.\n",
    "        # Note that you do not have to add relu after the last fc layer\n",
    "        model = []\n",
    "        self.net = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # place your code here\n",
    "\n",
    "\n",
    "class VAE2d(nn.Module):\n",
    "    def __init__(self, n_in, n_latent, enc_hidden_sizes, dec_hidden_sizes):\n",
    "        assert isinstance(enc_hidden_sizes, list)\n",
    "        assert isinstance(dec_hidden_sizes, list)\n",
    "        super().__init__()\n",
    "        self.n_latent = n_latent\n",
    "        self.encoder = FullyConnectedMLP(n_in, enc_hidden_sizes, 2 * n_latent)\n",
    "        self.decoder = FullyConnectedMLP(n_latent, dec_hidden_sizes, 2 * n_in)\n",
    "\n",
    "    def prior(self, n):\n",
    "        # return samples from prior distribtuion (standart normal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # place your code here\n",
    "        return mu_z, log_std_z, mu_x, log_std_x\n",
    "\n",
    "    def loss(self, x):\n",
    "        mu_z, log_std_z, mu_x, log_std_x = model(x)\n",
    "        # reconstruction loss is just logarithm of normal density distribution density \n",
    "        recon_loss = \n",
    "        recon_loss = recon_loss.sum(1).mean()\n",
    "        \n",
    "        # kl divergence of two normal distribution has analytic expression\n",
    "        # https://jhui.github.io/2017/03/06/Variational-autoencoders/\n",
    "        kl_loss = \n",
    "        kl_loss = kl_loss.sum(1).mean()\n",
    "\n",
    "        return {\n",
    "            'elbo_loss': recon_loss + kl_loss, \n",
    "            'recon_loss': recon_loss,\n",
    "            'kl_loss': kl_loss\n",
    "        }\n",
    "\n",
    "    def sample(self, n, noise=True):\n",
    "        with torch.no_grad():\n",
    "            z = self.prior(n)\n",
    "            mu, log_std = self.decoder(z).chunk(2, dim=1)\n",
    "            if noise:\n",
    "                z = torch.randn_like(mu) * log_std.exp() + mu\n",
    "            else:\n",
    "                z = mu\n",
    "        return z.cpu().numpy()\n",
    "\n",
    "\n",
    "def solve_task(train_data, test_data, model, batch_size, epochs, lr):\n",
    "    train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "    train_losses, test_losses = train_model(\n",
    "        model, train_loader, test_loader, epochs=EPOCHS, lr=LR, loss_key='elbo_loss'\n",
    "    )\n",
    "    samples_noise = model.sample(1000, noise=True)\n",
    "    samples_nonoise = model.sample(1000, noise=False)\n",
    "\n",
    "    for key, value in test_losses.items():\n",
    "        print('{}: {:.4f}'.format(key, value[-1]))\n",
    "\n",
    "    plot_training_curves(train_losses, test_losses)\n",
    "    visualize_2d_samples(samples_noise, title='Samples with Decoder Noise')\n",
    "    visualize_2d_samples(samples_nonoise, title='Samples without Decoder Noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDAWwEs8eJWV"
   },
   "outputs": [],
   "source": [
    "# choose these parameters\n",
    "\n",
    "ENC_HIDDEN_SIZES = \n",
    "DEC_HIDDEN_SIZES = \n",
    "BATCH_SIZE = \n",
    "EPOCHS = \n",
    "LR = \n",
    "COUNT = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IdJaAD6Ls4hL",
    "outputId": "aa052fcc-66e5-4ea3-a9c5-06e38240b5a1"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = generate_2d_data(COUNT, mode='multivariate')\n",
    "visualize_2d_data(train_data, test_data)\n",
    "\n",
    "model = VAE2d(2, 2, ENC_HIDDEN_SIZES, DEC_HIDDEN_SIZES).cuda()\n",
    "solve_task(train_data, test_data, model, BATCH_SIZE, EPOCHS, LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ob-yRJ8xe7Ns",
    "outputId": "86989571-6ee7-461b-d5a6-caa2832481d9"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = generate_2d_data(COUNT, mode='univariate')\n",
    "visualize_2d_data(train_data, test_data)\n",
    "\n",
    "model = VAE2d(2, 2, ENC_HIDDEN_SIZES, DEC_HIDDEN_SIZES).cuda()\n",
    "solve_task(train_data, test_data, model, BATCH_SIZE, EPOCHS, LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRCmH4X_6tyQ"
   },
   "source": [
    "After training the model on these 2 datasets, you have to see on \"Samples without Decoder Noise\" figures. This figures shows posterior means of the decoder. In the case of multivariate gaussian, the means are perfectly aligned with data distribution. \n",
    "Otherwise, in the of univariate gaussian you have to see strange figure. This happens due to so called \"posterior collapse\" (we will discuss it on the one of our lecture).\n",
    "Our posterior distribution $p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}))$ is a univariate (covariance matrix is diagonal). Thus, to model univariate data distribution (second case) the model ignores latent code, cause we could model it without any information from latent space.\n",
    "It is a real problem for generative models and we will discuss later how to overcome this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMACU__E8fmh"
   },
   "source": [
    "## Task 2: VAE on CIFAR10 data\n",
    "\n",
    "In this task you will implement VAE model for real image distribution. You do not have to change this functions (except the path to the data file, download it from here: https://drive.google.com/file/d/16j3nrJV821VOkkuRz7aYam8TyIXLnNme/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "I-jbbpNLvfbW",
    "outputId": "91d50891-f8b7-4e62-9b2f-19c8374cde78"
   },
   "outputs": [],
   "source": [
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    train_data, test_data = data['train'], data['test']\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def show_samples(samples, title, nrow=10):\n",
    "    samples = (torch.FloatTensor(samples) / 255).permute(0, 3, 1, 2)\n",
    "    grid_img = make_grid(samples, nrow=nrow)\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_data(data, title):\n",
    "    idxs = np.random.choice(len(data), replace=False, size=(100,))\n",
    "    images = train_data[idxs]\n",
    "    show_samples(images, title)\n",
    "\n",
    "train_data, test_data = load_pickle(os.path.join('drive', 'My Drive', 'DGM2020', 'cifar10.pkl'))\n",
    "visualize_data(train_data, 'CIFAR10 samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9iFs78D8uoE"
   },
   "source": [
    "Here the model specification will be almost the same with the following differences:\n",
    "* Now our encoder and decoder will be convolutional.\n",
    "* We do not model covariance matrix in posterior distribution (now it is identical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYI_WLXnsM4g"
   },
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, input_shape, n_latent):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.n_latent = n_latent\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "        )\n",
    "        conv_out_dim = input_shape[1] // 8 * input_shape[2] // 8 * 256\n",
    "        self.fc = nn.Linear(conv_out_dim, 2 * n_latent)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convs(x)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        mu, log_std = self.fc(out).chunk(2, dim=1)\n",
    "        return mu, log_std\n",
    "        \n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, n_latent, output_shape):\n",
    "        super().__init__()\n",
    "        self.n_latent = n_latent\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        self.base_size = (128, output_shape[1] // 8, output_shape[2] // 8)\n",
    "        self.fc = nn.Linear(n_latent, np.prod(self.base_size))\n",
    "        self.deconvs = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, output_shape[0], 3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.fc(z)\n",
    "        out = out.view(out.shape[0], *self.base_size)\n",
    "        return self.deconvs(out)\n",
    "\n",
    "\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, input_shape, n_latent):\n",
    "        super().__init__()\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.n_latent = n_latent\n",
    "        self.encoder = ConvEncoder(input_shape, n_latent)\n",
    "        self.decoder = ConvDecoder(n_latent, input_shape)\n",
    "\n",
    "    def prior(self, n):\n",
    "        return torch.randn(n, self.n_latent).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # place your code here\n",
    "        return mu_z, log_std_z, x_recon\n",
    "        \n",
    "    def loss(self, x):\n",
    "        mu_z, log_std_z, x_recon = model(x)\n",
    "        # use F.mse_loss for recon_loss\n",
    "        recon_loss = \n",
    "        kl_loss = \n",
    "\n",
    "        return {\n",
    "            'elbo_loss': recon_loss + kl_loss, \n",
    "            'recon_loss': recon_loss,\n",
    "            'kl_loss': kl_loss\n",
    "        }\n",
    "\n",
    "    def sample(self, n):\n",
    "        with torch.no_grad():\n",
    "            z = self.prior(n)\n",
    "            x_recon = self.decoder(z)\n",
    "            samples = torch.clamp(x_recon, -1, 1)\n",
    "        return samples.cpu().permute(0, 2, 3, 1).numpy() * 0.5 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "siM0O653zYQD",
    "outputId": "67788820-45e8-4ce0-c64c-73bb5aa599d6"
   },
   "outputs": [],
   "source": [
    "# choose these parameters\n",
    "\n",
    "BATCH_SIZE = \n",
    "EPOCHS = \n",
    "LR = \n",
    "\n",
    "train_data, test_data = load_pickle(os.path.join('drive', 'My Drive', 'DGM2020', 'cifar10.pkl'))\n",
    "\n",
    "train_data = (np.transpose(train_data, (0, 3, 1, 2)) / 255. * 2 - 1).astype('float32')\n",
    "test_data = (np.transpose(test_data, (0, 3, 1, 2)) / 255. * 2 - 1).astype('float32')\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = ConvVAE((3, 32, 32), 16).cuda()\n",
    "train_losses, test_losses = train_model(\n",
    "    model, train_loader, test_loader, epochs=EPOCHS, lr=LR, loss_key='elbo_loss'\n",
    ")\n",
    "samples = model.sample(100) * 255.\n",
    "\n",
    "x = next(iter(test_loader))[:50].cuda()\n",
    "with torch.no_grad():\n",
    "    z, _ = model.encoder(x)\n",
    "    x_recon = torch.clamp(model.decoder(z), -1, 1)\n",
    "reconstructions = torch.stack((x, x_recon), dim=1).view(-1, 3, 32, 32) * 0.5 + 0.5\n",
    "reconstructions = reconstructions.permute(0, 2, 3, 1).cpu().numpy() * 255\n",
    "\n",
    "x = next(iter(test_loader))[:20].cuda()\n",
    "with torch.no_grad():\n",
    "    z, _ = model.encoder(x)\n",
    "    z1, z2 = z.chunk(2, dim=0)\n",
    "    interps = [model.decoder(z1 * (1 - alpha) + z2 * alpha) for alpha in np.linspace(0, 1, 10)]\n",
    "    interps = torch.stack(interps, dim=1).view(-1, 3, 32, 32)\n",
    "    interps = torch.clamp(interps, -1, 1) * 0.5 + 0.5\n",
    "interps = interps.permute(0, 2, 3, 1).cpu().numpy() * 255\n",
    "\n",
    "samples, reconstructions, interps = samples.astype('float32'), reconstructions.astype('float32'), interps.astype('float32')\n",
    "for key, value in test_losses.items():\n",
    "    print('{}: {:.4f}'.format(key, value[-1]))\n",
    "plot_training_curves(train_losses, test_losses)\n",
    "show_samples(samples, title='Samples')\n",
    "show_samples(reconstructions, title='Reconstructions')\n",
    "show_samples(interps, title='Interpolations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIJoj_OY9LAU"
   },
   "source": [
    "## Task 3: RealNVP on 2d data\n",
    "\n",
    "In this task you will implement RealNVP modelon 2d moons dataset. Your do not have to change the data generation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wDA8Zo5SYbZ"
   },
   "outputs": [],
   "source": [
    "def generate_moons_data(count):\n",
    "    data, labels = make_moons(n_samples=count, noise=0.1)\n",
    "    data = data.astype('float32')\n",
    "    split = int(0.8 * count)\n",
    "    train_data, test_data = data[:split], data[split:]\n",
    "    train_labels, test_labels = labels[:split], labels[split:]\n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "Mh5vwoc-bgfE",
    "outputId": "ed1fdb0e-2293-4e1c-80a1-5f35dcbfe942"
   },
   "outputs": [],
   "source": [
    "COUNT = 5000\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = generate_moons_data(COUNT)\n",
    "visualize_2d_data(train_data, test_data, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8C4o9wx9x2D"
   },
   "source": [
    "See original paper for model description (https://arxiv.org/abs/1605.08803).\n",
    "\n",
    "The model is a sequence of affine coupning layers. Note that you have to permute the channels that will left unchanged between different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11DTN8IKB4DC"
   },
   "outputs": [],
   "source": [
    "class AffineTransform(nn.Module):\n",
    "    def __init__(self, partition_type, n_hiddens=[256, 256]):\n",
    "        super().__init__()\n",
    "        self.mask = self.build_mask(partition_type=partition_type)\n",
    "        self.scale = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "        self.scale_shift = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "        self.mlp = FullyConnectedMLP(input_shape=2, hiddens=n_hiddens, output_shape=2)\n",
    "\n",
    "    def build_mask(self, partition_type):\n",
    "        assert partition_type in {\"left\", \"right\"}\n",
    "        # place your code here\n",
    "\n",
    "    def forward(self, x, invert=False):\n",
    "        # place your code here (you have to mask x)\n",
    "\n",
    "        log_s, t = self.mlp(x_masked).split(1, dim=1)\n",
    "        # see original paper to understand these formulas\n",
    "        log_s = self.scale * torch.tanh(log_s) + self.scale_shift\n",
    "        t = t * (1.0 - mask)\n",
    "        log_s = log_s * (1.0 - mask)\n",
    "\n",
    "        #place your code here\n",
    "        # you have to create forward and inverse transforms (use invert flag)\n",
    "        return x, log_s\n",
    "\n",
    "\n",
    "class RealNVP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.prior = torch.distributions.Normal(torch.tensor(0.), torch.tensor(1.))\n",
    "        self.transforms = nn.ModuleList([\n",
    "            # place your code here (stack AffineTransform) with alternately changing partition_mode\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, invert=False):\n",
    "        z = x\n",
    "        log_det = 0.0\n",
    "        # place your code here (do not forget abour invert flag)\n",
    "        return z, log_det\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        # place your code here (use forward method)\n",
    "\n",
    "    def loss(self, x):\n",
    "        return {'nll_loss': -self.log_prob(x).mean()}\n",
    "\n",
    "    def sample(self, n):\n",
    "        # place your code here (sample from prior and put it to forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IobSqemwTgyB"
   },
   "outputs": [],
   "source": [
    "def show_2d_latents(latents, labels, title='Latent Space'):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.scatter(latents[:, 0], latents[:, 1], s=1, c=labels)\n",
    "    plt.xlabel('z1')\n",
    "    plt.ylabel('z2')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def show_2d_densities(densities, title='Densities'):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    dx, dy = 0.025, 0.025\n",
    "    x_lim = (-1.5, 2.5)\n",
    "    y_lim = (-1, 1.5)\n",
    "    y, x = np.mgrid[slice(y_lim[0], y_lim[1] + dy, dy),\n",
    "                    slice(x_lim[0], x_lim[1] + dx, dx)]\n",
    "    plt.pcolor(x, y, densities.reshape([y.shape[0], y.shape[1]]))\n",
    "    plt.pcolor(x, y, densities.reshape([y.shape[0], y.shape[1]]))\n",
    "    plt.xlabel('z1')\n",
    "    plt.ylabel('z2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 835
    },
    "id": "SNR4FHBATQMO",
    "outputId": "472a63f2-c4a5-4eb2-b251-e57fcf7a5cb9"
   },
   "outputs": [],
   "source": [
    "# choose these parameters\n",
    "\n",
    "BATCH_SIZE = \n",
    "EPOCHS = \n",
    "LR = \n",
    "COUNT = 5000\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = generate_moons_data(COUNT)\n",
    "\n",
    "loader_args = dict(batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader = data.DataLoader(train_data, **loader_args)\n",
    "test_loader = data.DataLoader(test_data, **loader_args)\n",
    "\n",
    "# model\n",
    "real_nvp = RealNVP().cuda()\n",
    "\n",
    "# train\n",
    "train_losses, test_losses = train_model(\n",
    "    real_nvp, train_loader, test_loader, epochs=EPOCHS, lr=LR, loss_key='nll_loss'\n",
    ")\n",
    "\n",
    "# heatmap\n",
    "dx, dy = 0.025, 0.025\n",
    "x_lim = (-1.5, 2.5)\n",
    "y_lim = (-1, 1.5)\n",
    "y, x = np.mgrid[slice(y_lim[0], y_lim[1] + dy, dy),\n",
    "                slice(x_lim[0], x_lim[1] + dx, dx)]\n",
    "mesh_xs = torch.FloatTensor(np.stack([x, y], axis=2).reshape(-1, 2)).cuda()\n",
    "densities = np.exp(real_nvp.log_prob(mesh_xs).cpu().detach().numpy())\n",
    "\n",
    "# latents\n",
    "z = real_nvp(torch.FloatTensor(train_data).cuda())[0]\n",
    "latents = z.cpu().detach().numpy()\n",
    "\n",
    "plot_training_curves(train_losses, test_losses)\n",
    "show_2d_densities(densities)\n",
    "show_2d_latents(latents, train_labels)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
